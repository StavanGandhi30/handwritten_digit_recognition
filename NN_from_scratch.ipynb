{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d315143f-d098-4719-8b49-28922eaae4d7",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263d728b-0b34-4f5f-9762-a5265541ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version =  1.26.4\n",
      "Pandas version =  2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Numpy version = \", np.__version__) # 1.26.4\n",
    "print(\"Pandas version = \", pd.__version__) # 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02fb835-a2aa-489a-abe4-9882c4019de2",
   "metadata": {},
   "source": [
    "# Load dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86498099-50ad-44b9-ad35-53d4db7c57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/train.csv') # load csv data into a data variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f50f758-a51c-4830-9e7c-3845bcccbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data) # convert pandas data to numpy array\n",
    "m, n = data.shape # (Images, 1 label + 784 pixels) = (42000, 785)\n",
    "np.random.shuffle(data) # shuffle so we get random train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9756fdc3-1da0-4387-ae0e-84fe935c0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data[:1000] # split first 1000 data for testing \n",
    "data_dev = data_dev.T # We transpose the data here (data.T) to align the dimensions properly for mathematical operations and better visualization\n",
    "Y_dev = data_dev[0] # split labels from those 1000 data\n",
    "X_dev = data_dev[1:n] # split pixels from those 1000 data \n",
    "X_dev = X_dev / 255. # convert 0-255 pixel values to 0.0-1.0 values\n",
    "\n",
    "data_train = data[1000:] # split everything besides first 1000 data for training\n",
    "data_train = data_train.T # We transpose the data here (data.T) to align the dimensions properly for mathematical operations and better visualization\n",
    "Y_train = data_train[0] # split labels from those 1000 data\n",
    "X_train = data_train[1:n] # split pixels from those 1000 data\n",
    "X_train = X_train / 255. # convert 0-255 pixel values to 0.0-1.0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71875214-2d51-4fb7-90ff-5893e59885e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 41000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e7801-8222-43f9-b1e6-0f5051197b15",
   "metadata": {},
   "source": [
    "# Init Paramater\n",
    "#### To initialize the parameters their are few algorithms that we could use depending purpose and activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55acb1-8f38-4165-a5c6-67ce3ea89e8f",
   "metadata": {},
   "source": [
    "\n",
    "##### If your weights never go negative, your model would be severely limited — it could only add influences and never subtract them. That would hurt performance, especially in more complex problems.\n",
    "Weights determine how strongly a neuron responds to its inputs.\n",
    "> - Negative weight = the neuron is inhibited by that input.\n",
    "> - Positive weight = the neuron is activated by that input.\n",
    "\n",
    "Biases shift the activation threshold:\n",
    "> - A negative bias can make it harder for the neuron to activate.\n",
    "> - A positive bias can make it easier to activate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6c0e921-5cf2-41e4-854f-1f13f9d76490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh / sigmoid\n",
    "# This will output numpy array of shape (output_nodes, input_nodes) where \n",
    "# For 784 input nodes + 10 output nodes: each values will be between (-0.00755667506 to 0.00755667506)\n",
    "# For 10 input nodes + 10 output nodes: each values will be between (-0.3 to 0.3)\n",
    "def xavier_init(input_nodes, output_nodes):\n",
    "    limit = np.sqrt(6 / (input_nodes + output_nodes))\n",
    "    return np.random.uniform(-limit, limit, size=(output_nodes, input_nodes))\n",
    "\n",
    "# ReLU / LeakyReLU\n",
    "# This will output numpy array of shape (output_nodes, input_nodes) where \n",
    "# For 784 input nodes: each values will be between (0 to 0.0025510204)\n",
    "# For 10 input nodes: each values will be between (0 to 0.2)\n",
    "def he_init(input_nodes, output_nodes):\n",
    "    std = np.sqrt(2 / input_nodes)\n",
    "    return np.random.randn(output_nodes, input_nodes) * std\n",
    "\n",
    "# selu\n",
    "# This will output numpy array of shape (output_nodes, input_nodes) where \n",
    "# For 784 input nodes: each values will be between (0 to 0.0012755102)\n",
    "# For 10 input nodes: each values will be between (0 to 0.1)\n",
    "def lecun_init(input_nodes, output_nodes):\n",
    "    std = np.sqrt(1 / input_nodes)\n",
    "    return np.random.randn(output_nodes, input_nodes) * std\n",
    "\n",
    "# Ignores Activation Functions\n",
    "# This will output numpy array of shape (output_nodes, input_nodes) where each values are between (-0.5 to 0.5)\n",
    "def uniform_random_init(input_nodes, output_nodes):\n",
    "    return np.random.rand(output_nodes, input_nodes) - 0.5\n",
    "\n",
    "# This will output numpy array of shape (output_nodes, input_nodes) where each value is 0.\n",
    "def zero_init(input_nodes, output_nodes):\n",
    "    return np.zeros((input_nodes, output_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fe1848-2077-456f-bdcd-e9b2ab0d211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(method=\"he\"):\n",
    "    if method==\"he\":\n",
    "        W1 = he_init(784, 10)\n",
    "        W2 = he_init(10, 10)\n",
    "    elif method==\"xavier\":\n",
    "        W1 = xavier_init(784, 10)\n",
    "        W2 = xavier_init(10, 10)\n",
    "    elif method==\"lecun\":\n",
    "        W1 = lecun_init(784, 10)\n",
    "        W2 = lecun_init(10, 10)\n",
    "    else:\n",
    "        W1 = uniform_random_init(784, 10)\n",
    "        W2 = uniform_random_init(10, 10)\n",
    "\n",
    "    b1 = zero_init(10, 1)\n",
    "    b2 = zero_init(10, 1)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20351351-f687-43b7-844e-5422485f5e0c",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "#### Without activation functions, a neural network would just be a linear function — a stack of linear layers is still just linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a31bf4-dac7-40cf-9ef1-65750d6d00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties: Output: (0, 1), Smooth & differentiable, Used historically in binary classification\n",
    "# Problem: Vanishing gradient for large |x|, Activations saturate (output close to 0 or 1), Non-zero-centered (bad for gradient updates)\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Properties: Output: (-1, 1), Zero-centered: better than sigmoid\n",
    "# Problem: Vanishing gradient for large |x|\n",
    "def tanh(Z)\n",
    "    return np.tanh(Z) # (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "# Properties: Fast & simple, Sparse activation (many zeros), Avoids vanishing gradient for positive values\n",
    "# Problem: Dying ReLU problem: neurons can “die” (output zero) and never recover\n",
    "def reLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "# Fixes dying ReLU by allowing small gradient when x < 0\n",
    "def lReLU(Z, alpha=0.01):\n",
    "    return np.maximum(Z, alpha * Z)\n",
    "\n",
    "# Properties: Smooth (vs. ReLU's sharp corner), Can push mean activation closer to 0 (like BatchNorm)\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DigitRecog",
   "language": "python",
   "name": "digitrecog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
